---
title: "Ejercicios Método de Montecarlo"
subtitle: "Técnicas de Computación para la Estadística"
author: "Marta Venegas Pardo"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
set.seed(457367)
```



Para crear éste documento RMarkdown reproducible, hemos hecho uso de una
semilla.

Para cada uno de los problemas que se proponen, calcular la estimación
requerida mediante el método de Montecarlo. Construir el mejor (de menor
longitud) intervalo de confianza con probabilidad de cobertura 99.7% que
se pueda.

Para tratar de mejorar los intervalos construidos pueden aplicarse las
siguientes técnicas:

-   Generar una mayor cantidad de valores aleatorios (considerar un $n$
    mayor).

-   Aplicar técnicas de reducción de varianza: variables antitéticas,
    muestreo estratificado, muestreo por importancia.

Documentar en RMarkdown (de manera reproducible y con parámetros, en su
caso) todo el proceso de construcción y mejora del intervalo de
confianza.

Puede ser de utilidad considerar esta página de Wikipedia, en la que
puede encontrarse un listado de distintas distribuciones de
probabilidad, y este paquete de R, que permite manejar distribuciones de
probabilidad adicionales a las estándar.

Los problemas propuestos son los siguientes:

# Ejercicio 1

Estimarr el valor de: $$I =
  \int_{0}^{1} x^2 \sqrt{1-x^2}  \,dx = \dfrac{\pi}{16} \simeq 0,1963495408493621$$

## Método Montecarlo
Para estimar esta integral mediante el Método de Montecarlo, hay que
poner la integral como una esperanza de x.

Es decir: $I=E[g(X)]$, donde X es una variable uniforme en el intervalo
(0,1) y g(x) = $x^2 \sqrt{1-x^2}$.



1. Establecemos la forma de generar valores aleatorios, en este caso de manera uniforme en el intervalo \((0, 1)\).

2. Definimos la función \( g \) que deberá aplicarse a esos valores aleatorios.

```{r generacion}
genera_valor_aleatorio <- function() {
  runif(1)
}



g <- function(x) {
  sqrt((1 - x^2))*x^2
}
```

3. Replicamos el proceso de generar un valor aleatorio y aplicarle la función \( g \). La variable `n` determina la cantidad de replicaciones.

```{r replicacion}
n <- 1e5
valores_g <- replicate(n , {
  x <- genera_valor_aleatorio()
  g(x)
}
)
```



4. Calculamos una estimación de \( I_{1} \) a partir de la media aritmética de los valores obtenidos.

```{r}
estimacion <- mean(valores_g)
```

El valor estimado de $I$ es $`r estimacion`$. Este método siempre dará
una aproximación, mejor o peor pero se trata de una aproximación.

Puesto que esto es un resultado aleatorio, hay que calcular siempre un
intervalo de confianza que permita acotar la variabilidad de la
estimación.


```{r}
probabilidad_cobertura <- 0.997
alfa <- 1 - probabilidad_cobertura
percentil<- qnorm(1 - alfa / 2)
error_estandar <- sqrt(var(valores_g)/n)
intervalo_confianza <- estimacion + c(-1,1) * error_estandar * percentil
```

Luego, un intervalo de confianza con probabilidad de cobertura 0.997 es
( $`r intervalo_confianza`$ )

## Reducción de Varianza por Antitéticas


Cáculamos la varianza y la eficiencia del Método de Newton para compararlo con las demás varianzas y eficiencias.

Estimaremos también el coste en tiempo del método haciendo uso de las herramientas proporcionadas por el paquete `bench` (en particular, la función `mark` analiza el coste en tiempo y en memoria de las expresiones proporcionadas, ejecutando cada una de ellas un cierto número de iteraciones y devolviendo una tabla con distintas medidas, entre ellas la mediana de los tiempos de ejecución de cada iteración). De esta forma, podremos estimar la eficiencia del método directo a la hora de estimar el valor de la integral.

```{r Montecarlo-directo, warning=FALSE}
genera_valor_aleatorio <- function() {
  runif(1)
}

g <- function(x) {
  (x^2)*(sqrt(1 - x^2))
}

n <- 1e4
coste_directo <- bench::mark({
  valores_g <- replicate(n, {
    x <- genera_valor_aleatorio()
    g(x)
  })
},
iterations = 10,
time_unit = "ms"
)$median

estimacion_directo <- mean(valores_g)
varianza_directo <- var(valores_g) / n

eficiencia_directo <- 1 / (varianza_directo * coste_directo)
```


A continuación vamos a aplicar el método de las variables antitéticas para tratar de reducir la varianza de la estimación.

```{r generacion-antiteticas}
genera_valores_antiteticas <- function() {
  runif(1)
}


g <- function(x) {
  sqrt((1 - x^2))*x^2
}
```

Ahora replicamos el proceso de generar valores antitéticos, aplicarles la función `g` a cada uno de ellos y calcular el promedio. Para que los resultados se puedan comparar con los del método directo, es necesario generar solo `n / 2` pares de valores antitéticos (para que se hayan generado `n` valores aleatorios en total). Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-antiteticas, warning=FALSE}
coste_antiteticas <- bench::mark({ # para ver cuanto tarda
  valores <- replicate(n / 2, {
    valores_antiteticos <- genera_valores_antiteticas()
    mean(g(valores_antiteticos)) # (xi + yi) / 2
  })
},
iterations = 10,
time_unit = "ms"
)$median
```

Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

No calculo IC ni nada porque aquí no me interesa, en el examen si tendremos que hacerlo.

```{r estimacion-antiteticas}
estimacion_antiteticas <- mean(valores)
varianza_antiteticas <- var(valores) / (n / 2) # Cuasivar muestral
eficiencia_antiteticas <- 1 / (varianza_antiteticas * coste_antiteticas)
```



La siguiente tabla compara los resultados obtenidos por el método directo de Montecarlo y por el método de las variables antitéticas.

```{r tabla-de-resultados-antiteticas}
knitr::kable(
  data.frame(
    `Método` = c("Directo", "Antitéticas"),
    `Estimación` = c(estimacion_directo, estimacion_antiteticas),
    Varianza = c(varianza_directo, varianza_antiteticas),
    Coste = c(coste_directo, coste_antiteticas),
    Eficiencia = c(eficiencia_directo, eficiencia_antiteticas)
  ),
  digits = 10
)
```

Se puede observar cómo la varianza se reduce en un factor de 10, con un aumento del coste en tiempo que puede llegar a ser considerable. Además, se observa que el método de las antitéticas es menos eficiente que el directo.


## Reducción de la varianza por Muestreo Estratificado:

A continuación vamos a aplicar el método del muestreo estratificado para tratar de reducir la varianza de la estimación.


```{r grafica-g.Est}
library(ggplot2)

ggplot2::ggplot() +
  geom_function(fun = g) +
  xlim(0, 1)
```

La representación gráfica de la función \( g \) muestra que unos estratos adecuados podrían ser los subintervalos \( (0, 4/5) \) y \( (4/5, 1) \).

### Muestreo estratificado: asignación proporcional


1. En primer lugar definimos los estratos y establecemos la forma de generar valores aleatorios dentro de cada estrato. Una manera simple de definir los estratos es subdividir el intervalo (0, 1) de forma independiente en cada dimensión.

```{r generacion-Mest}
estratos <- data.frame(
  min = c(0, 4 / 5),
  max = c(4/ 5, 1),
  probabilidad = c(4 / 5, 1 / 5)
)
# Función que genera valores dentro de un estrato
genera_valor_en_estrato <- function(numero_estrato) {
  estrato <- estratos[numero_estrato, ]
  runif(1, min = estrato$min, max = estrato$max)
}
```

2. Ahora replicamos el proceso de generar valores en cada estrato, en una cantidad proporcional a su probabilidad, y aplicarles la función `g` a cada uno de ellos. Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-proporcionalMest, warning=FALSE}
unidad_de_tiempo <- "ms"
n_estratos <- n * estratos$probabilidad
# Aseguramos valores enteros
n_estratos <- ceiling(n_estratos)
# Aseguramos al menos dos valores en cada estrato
n_estratos <- pmax(n_estratos, 2)

coste_estratificado_proporcional <- bench::mark(
  {
    valores <- lapply(
      seq_len(2), # (1,2), aplico la función 2 veces, una para cada estrato
      function(numero_estrato) {
        replicate(n_estratos[numero_estrato], {
          x <- genera_valor_en_estrato(numero_estrato)
          g(x)
        })
      }
    )
  },
  iterations = 10,
  time_unit = unidad_de_tiempo
)$median
```

Se han generado \( `r n_estratos[1]` \) valores en el primer estrato y \( `r n_estratos[2]` \) valores en el segundo estrato.

3. Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

```{r estimacion-proporcionalMest}
estimacion_estratificado_proporcional <-
  weighted.mean(
    sapply(valores, mean),
    estratos$probabilidad
  )
varianza_estratificado_proporcional <-
  sum(estratos$probabilidad^2 * sapply(valores, var) / n_estratos)
eficiencia_estratificado_proporcional <-
  1 / (varianza_estratificado_proporcional *
    coste_estratificado_proporcional)
```


### Muestreo estratificado: asignación óptima

Consideramos los mismos estratos que antes y, por tanto, la misma forma de generar valores en cada uno de ellos.

Ahora replicamos el proceso de generar en cada estrato una cantidad óptima de valores, determinada mediante un procedimiento en dos etapas, y aplicarles la función `g` a cada uno de ellos. Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-optimo, warning=FALSE}
n_tanteo <- 100
n_produccion <- n - n_tanteo # Para que la cantidad total de valores generados sea igual en los tres métodos y, por tanto, su comparación tenga sentido

coste_estratificado_optimo <- bench::mark(
  {
    # Estimación de las varianzas de los estratos
    n_estratos <- pmax(
      ceiling(n_tanteo * estratos$probabilidad), # asignación proporcional para calcular la var
      2
    )
    valores <- lapply(
      seq_len(2),
      function(numero_estrato) {
        replicate(n_estratos[numero_estrato], {
          x <- genera_valor_en_estrato(numero_estrato)
          g(x)
        })
      }
    )

    # Cantidad óptima de valores en cada estrato
    sigmas <- sapply(valores, sd)
    n_estratos <-
      pmax(
        ceiling(n_produccion * estratos$probabilidad * sigmas /
          sum(estratos$probabilidad * sigmas)),
        2
      )

    # Generación de valores en cada estrato
    valores <- lapply(
      seq_len(2),
      function(numero_estrato) {
        replicate(n_estratos[numero_estrato], {
          x <- genera_valor_en_estrato(numero_estrato)
          g(x)
        })
      }
    )
  },
  iterations = 10,
  time_unit = unidad_de_tiempo
)$median
```

Se han generado \( `r n_estratos[1]` \) valores en el primer estrato y \( `r n_estratos[2]` \) valores en el segundo estrato.

Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

```{r estimacion-optimo}
estimacion_estratificado_optimo <-
  weighted.mean(
    sapply(valores, mean),
    estratos$probabilidad
  )
varianza_estratificado_optimo <-
  sum(estratos$probabilidad^2 * sapply(valores, var) / n_estratos)
eficiencia_estratificado_optimo <-
  1 / (varianza_estratificado_optimo *
    coste_estratificado_optimo)
```

La siguiente tabla compara los resultados obtenidos por el método directo de Montecarlo y por el método del muestreo estratificado.

```{r tabla-de-resultados-est}
knitr::kable(
  data.frame(
    `Método` = c(
      "Directo",
      "Estratificado proporcional",
      "Estratificado óptimo"
    ),
    `Estimación` = c(
      estimacion_directo,
      estimacion_estratificado_proporcional,
      estimacion_estratificado_optimo
    ),
    Varianza = c(
      varianza_directo,
      varianza_estratificado_proporcional,
      varianza_estratificado_optimo
    ),
    Coste = c(
      coste_directo,
      coste_estratificado_proporcional,
      coste_estratificado_optimo
    ),
    Eficiencia = c(
      eficiencia_directo,
      eficiencia_estratificado_proporcional,
      eficiencia_estratificado_optimo
    )
  ),
  digits = 10
)
```

Para este problema, la eficiencia del método del muestreo estratificado es mucho menor que la del método directo, ya que la reducción de varianza conseguida es anulada completamente por el aumento del coste en tiempo. En este caso, el Método Estratificado proporcional es mejor, ya que reduce la varianza en un factor de 10 y además es el que menor eficiencia tiene.


## Reducción de la varianza por Muestreo por Importancia:


A continuación vamos a aplicar el método del muestreo por importancia para tratar de reducir la varianza de la estimación.

Buscamos una densidad instrumental con soporte al menos en el intervalo \( (0, 1) \) y que sea lo más parecida posible a la función \( |g(x)| f_{1}(x) \), donde \( f_{1} \) es la densidad de \( \mathrm{U}(0, 1) \). Puesto que la función de densidad de una distribución beta de parámetros \( \alpha \) y \( \beta \) es \( f_2(x) \propto x^{\alpha - 1} (1 - x)^{\beta - 1} \), con soporte en \( (0, 1) \), parece conveniente usar como densidad instrumental la de una distribución beta con \( \alpha = 3 \) y \( \beta = \frac{1}{2} + 1 = \frac{3}{2} \).

```{r grafica-Imp}
library(ggplot2)

alfa <- 3
beta <- 3 / 2
ggplot(data.frame()) +
  geom_function(fun = g, aes(colour = "g")) +
  geom_function(fun = dunif, aes(colour = "unif")) +
  geom_function(
    fun = dbeta,
    args = list(shape1 = alfa, shape2 = beta),
    aes(colour = "beta")
  ) +
  scale_colour_manual("",
    values = c(g = "black", unif = "blue", beta = "red"),
    breaks = c("g", "unif", "beta")
  ) +
  xlim(0, 1)
```

En primer lugar, establecemos la forma de generar valores aleatorios, que ahora será a partir de la distribución beta escogida. Por otra parte, el producto de \( g \) por la razón de verosimilitud (es decir, el cociente entre la función de densidad de la distribución uniforme y la función de densidad de la distribución beta escogida) es el siguiente:

Recuerda, la función de densidad de la distribución beta es la siguiente:

$$X \sim \beta(\alpha,\beta), \quad f_X(x)= \dfrac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$$

Además,
\[\quad B(\alpha,\beta)= \dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}
\]

Por tanto:


$$
  g(x) \frac{1}{\frac{x^{3 - 1} (1 - x)^{3/2 - 1}}{B(3, 3/2)}}
  = \frac{\Gamma(3) \Gamma(3/2)}{\Gamma(5/2)} {x^{3 - 1} (1 - x)^{3/2 - 1}}
$$


```{r generacion-Imp}
genera_valor_aleatorio <- function() {
  rbeta(1, shape1 = alfa, shape2 = beta)
}

g_por_verosimilitud <- function(x) {
  (gamma(3)*gamma(3/2)/gamma(5/2))*(x^2)*(sqrt(1-x))
}
```

A continuación, replicamos `n` veces el proceso de generación de valores. Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-Imp, warning=FALSE}
coste_importancia <- bench::mark(
  {
    valores <- replicate(n, {
      x <- genera_valor_aleatorio()
      g_por_verosimilitud(x)
    })
  },
  iterations = 10,
  time_unit = unidad_de_tiempo
)$median
```

Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

```{r estimacion-Imp}
estimacion_importancia <- mean(valores)
varianza_importancia <- var(valores) / n
eficiencia_importancia <- 1 / (varianza_importancia * coste_importancia)
```

La siguiente tabla compara los resultados obtenidos por el método directo de Montecarlo y por el método del muestreo por importancia.

```{r tabla-de-resultados-Imp}
knitr::kable(
  data.frame(
    `Método` = c(
      "Directo",
      "Importancia"
    ),
    `Estimación` = c(
      estimacion_directo,
      estimacion_importancia
    ),
    Varianza = c(
      varianza_directo,
      varianza_importancia
    ),
    Coste = c(
      coste_directo,
      coste_importancia
    ),
    Eficiencia = c(
      eficiencia_directo,
      eficiencia_importancia
    )
  ),
  digits = 10
)
```

Se puede observar cómo la varianza se reduce en un factor de \( 10 \), con solo un pequeño aumento del coste en tiempo y con un gran aumento en la eficiencia. Esto quiere decir que, escogiendo una densidad instrumental adecuada, el método del muestreo por importancia es más eficiente que el método directo cuando se trata de estimar el valor de \( I \).

## Tabla general de resultados:

```{r tabla-de-resultados-final}
knitr::kable(
  data.frame(
    `Método` = c("Directo", "Antitéticas","Estratificado proporcional",
      "Estratificado óptimo","Importancia"),
    `Estimación` = c(estimacion_directo, estimacion_antiteticas,estimacion_estratificado_proporcional,
      estimacion_estratificado_optimo,estimacion_importancia),
    Varianza = c(varianza_directo, varianza_antiteticas,varianza_estratificado_proporcional,
      varianza_estratificado_optimo,varianza_importancia),
    Coste = c(coste_directo, coste_antiteticas,coste_estratificado_proporcional,
      coste_estratificado_optimo,coste_importancia),
    Eficiencia = c(eficiencia_directo, eficiencia_antiteticas,eficiencia_estratificado_proporcional,
      eficiencia_estratificado_optimo,eficiencia_importancia)
    
  ),
  digits = 10
)
```










# Ejercicio 2

Estimarr el valor de: $$I =
  \int_{0}^{1} -30 \cos(\pi x) x^4 (1-x^4)  \,dx  \simeq 1,714716594386961$$


Para estimar la primera integral mediante el método de Montecarlo, basta observar que \( I_{1} = \mathbb{E}[g(X)] \), donde \( X \) es una variable uniforme en el intervalo \( (0, 1) \) y \( g(x) = \int_{0}^{1} x^{2}{\sqrt{1-x^{2}}} \mathop{}\!d x \).

1. Establecemos la forma de generar valores aleatorios, en este caso de manera uniforme en el intervalo \((0, 1)\).

2. Definimos la función \( g \) que deberá aplicarse a esos valores aleatorios.




```{r generacion}
pi<- pi
g<- function(x){
  -30 * cos(pi * x) * x^4 * (1-x^4)
}
```



3. Replicamos el proceso de generar un valor aleatorio y aplicarle la función \( g \). La variable `n` determina la cantidad de replicaciones.

```{r replicacion}
n <- 1e6
valores_g <- replicate(n , {
  x <- genera_valor_aleatorio()
  g(x)
}
)
```

4. Calculamos una estimación de \( I_{1} \) a partir de la media aritmética de los valores obtenidos.


```{r estimacion}
estimacion <- mean(valores_g)
```

El valor estimado de $I$ es $`r estimacion`$. Este método siempre dará
una aproximación, mejor o peor pero se trata de una aproximación.

Puesto que esto es un resultado aleatorio, hay que calcular siempre un
intervalo de confianza que permita acotar la variabilidad de la
estimación.

5. Intervalo de confianza

```{r intervalo-de-confianza}
probabilidad_cobertura <- 0.997
alfa <- 1 - probabilidad_cobertura
percentil<- qnorm(1 - alfa / 2)
error_estandar <- sqrt(var(valores_g)/n)
intervalo_confianza <- estimacion + c(-1,1) * error_estandar * percentil
```

Luego, un intervalo de confianza con probabilidad de cobertura 0.997 es
( $`r intervalo_confianza`$ )


## Reducción de Varianza por Antitéticas


Cáculamos la varianza y la eficiencia del Método de Newton para compararlo con las demás varianzas y eficiencias.

Estimaremos también el coste en tiempo del método haciendo uso de las herramientas proporcionadas por el paquete `bench` (en particular, la función `mark` analiza el coste en tiempo y en memoria de las expresiones proporcionadas, ejecutando cada una de ellas un cierto número de iteraciones y devolviendo una tabla con distintas medidas, entre ellas la mediana de los tiempos de ejecución de cada iteración). De esta forma, podremos estimar la eficiencia del método directo a la hora de estimar el valor de la integral.

```{r Montecarlo-directo, warning=FALSE}
genera_valor_aleatorio <- function() {
  runif(1)
}

pi<- pi
g<- function(x){
  -30 * cos(pi * x) * x^4 * (1-x^4)
}

n <- 1e4
coste_directo <- bench::mark({
  valores_g <- replicate(n, {
    x <- genera_valor_aleatorio()
    g(x)
  })
},
iterations = 10,
time_unit = "ms"
)$median

estimacion_directo <- mean(valores_g)
varianza_directo <- var(valores_g) / n

eficiencia_directo <- 1 / (varianza_directo * coste_directo)
```


A continuación vamos a aplicar el método de las variables antitéticas para tratar de reducir la varianza de la estimación.

```{r generacion}
genera_valores_antiteticas <- function() {
  runif(1)
}


g<- function(x){
  -30 * cos(pi * x) * x^4 * (1-x^4)
}
```

Ahora replicamos el proceso de generar valores antitéticos, aplicarles la función `g` a cada uno de ellos y calcular el promedio. Para que los resultados se puedan comparar con los del método directo, es necesario generar solo `n / 2` pares de valores antitéticos (para que se hayan generado `n` valores aleatorios en total). Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-antiteticas, warning=FALSE}
coste_antiteticas <- bench::mark({ # para ver cuanto tarda
  valores <- replicate(n / 2, {
    valores_antiteticos <- genera_valores_antiteticas()
    mean(g(valores_antiteticos)) # (xi + yi) / 2
  })
},
iterations = 10,
time_unit = "ms"
)$median
```

Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

No calculo IC ni nada porque aquí no me interesa, en el examen si tendremos que hacerlo.

```{r estimacion-antiteticas}
estimacion_antiteticas <- mean(valores)
varianza_antiteticas <- var(valores) / (n / 2) # Cuasivar muestral
eficiencia_antiteticas <- 1 / (varianza_antiteticas * coste_antiteticas)
```



La siguiente tabla compara los resultados obtenidos por el método directo de Montecarlo y por el método de las variables antitéticas.

```{r tabla-de-resultados-antiteticas}
knitr::kable(
  data.frame(
    `Método` = c("Directo", "Antitéticas"),
    `Estimación` = c(estimacion_directo, estimacion_antiteticas),
    Varianza = c(varianza_directo, varianza_antiteticas),
    Coste = c(coste_directo, coste_antiteticas),
    Eficiencia = c(eficiencia_directo, eficiencia_antiteticas)
  ),
  digits = 10
)
```

Se puede observar cómo la varianza se reduce en un factor de 10, con un aumento del coste en tiempo que puede llegar a ser considerable. Además, se observa que el método de las antitéticas es menos eficiente que el directo.


## Reducción de la varianza por Muestreo Estratificado:

A continuación vamos a aplicar el método del muestreo estratificado para tratar de reducir la varianza de la estimación.


```{r grafica-g.Est}
library(ggplot2)

ggplot2::ggplot() +
  geom_function(fun = g) +
  xlim(0, 1)
```

La representación gráfica de la función \( g \) muestra que unos estratos adecuados podrían ser los subintervalos \( (0, 4/5) \) y \( (4/5, 1) \).

### Muestreo estratificado: asignación proporcional


1. En primer lugar definimos los estratos y establecemos la forma de generar valores aleatorios dentro de cada estrato. Una manera simple de definir los estratos es subdividir el intervalo (0, 1) de forma independiente en cada dimensión.

```{r generacion-Mest}
estratos <- data.frame(
  min = c(0, 4 / 5),
  max = c(4/ 5, 1),
  probabilidad = c(4 / 5, 1 / 5)
)
# Función que genera valores dentro de un estrato
genera_valor_en_estrato <- function(numero_estrato) {
  estrato <- estratos[numero_estrato, ]
  runif(1, min = estrato$min, max = estrato$max)
}
```

2. Ahora replicamos el proceso de generar valores en cada estrato, en una cantidad proporcional a su probabilidad, y aplicarles la función `g` a cada uno de ellos. Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-proporcionalMest, warning=FALSE}
unidad_de_tiempo <- "ms"
n_estratos <- n * estratos$probabilidad
# Aseguramos valores enteros
n_estratos <- ceiling(n_estratos)
# Aseguramos al menos dos valores en cada estrato
n_estratos <- pmax(n_estratos, 2)

coste_estratificado_proporcional <- bench::mark(
  {
    valores <- lapply(
      seq_len(2), # (1,2), aplico la función 2 veces, una para cada estrato
      function(numero_estrato) {
        replicate(n_estratos[numero_estrato], {
          x <- genera_valor_en_estrato(numero_estrato)
          g(x)
        })
      }
    )
  },
  iterations = 10,
  time_unit = unidad_de_tiempo
)$median
```

Se han generado \( `r n_estratos[1]` \) valores en el primer estrato y \( `r n_estratos[2]` \) valores en el segundo estrato.

3. Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

```{r estimacion-proporcionalMest}
estimacion_estratificado_proporcional <-
  weighted.mean(
    sapply(valores, mean),
    estratos$probabilidad
  )
varianza_estratificado_proporcional <-
  sum(estratos$probabilidad^2 * sapply(valores, var) / n_estratos)
eficiencia_estratificado_proporcional <-
  1 / (varianza_estratificado_proporcional *
    coste_estratificado_proporcional)
```


### Muestreo estratificado: asignación óptima

Consideramos los mismos estratos que antes y, por tanto, la misma forma de generar valores en cada uno de ellos.

Ahora replicamos el proceso de generar en cada estrato una cantidad óptima de valores, determinada mediante un procedimiento en dos etapas, y aplicarles la función `g` a cada uno de ellos. Haremos también uso del paquete `bench` para estimar el coste en tiempo del método, para poder así estimar su eficiencia a la hora de estimar el valor de la integral.

```{r replicacion-optimo, warning=FALSE}
n_tanteo <- 100
n_produccion <- n - n_tanteo # Para que la cantidad total de valores generados sea igual en los tres métodos y, por tanto, su comparación tenga sentido

coste_estratificado_optimo <- bench::mark(
  {
    # Estimación de las varianzas de los estratos
    n_estratos <- pmax(
      ceiling(n_tanteo * estratos$probabilidad), # asignación proporcional para calcular la var
      2
    )
    valores <- lapply(
      seq_len(2),
      function(numero_estrato) {
        replicate(n_estratos[numero_estrato], {
          x <- genera_valor_en_estrato(numero_estrato)
          g(x)
        })
      }
    )

    # Cantidad óptima de valores en cada estrato
    sigmas <- sapply(valores, sd)
    n_estratos <-
      pmax(
        ceiling(n_produccion * estratos$probabilidad * sigmas /
          sum(estratos$probabilidad * sigmas)),
        2
      )

    # Generación de valores en cada estrato
    valores <- lapply(
      seq_len(2),
      function(numero_estrato) {
        replicate(n_estratos[numero_estrato], {
          x <- genera_valor_en_estrato(numero_estrato)
          g(x)
        })
      }
    )
  },
  iterations = 10,
  time_unit = unidad_de_tiempo
)$median
```

Se han generado \( `r n_estratos[1]` \) valores en el primer estrato y \( `r n_estratos[2]` \) valores en el segundo estrato.

Finalmente, estimamos el valor de la integral, la varianza de esa estimación y la eficiencia del método.

```{r estimacion-optimo}
estimacion_estratificado_optimo <-
  weighted.mean(
    sapply(valores, mean),
    estratos$probabilidad
  )
varianza_estratificado_optimo <-
  sum(estratos$probabilidad^2 * sapply(valores, var) / n_estratos)
eficiencia_estratificado_optimo <-
  1 / (varianza_estratificado_optimo *
    coste_estratificado_optimo)
```

La siguiente tabla compara los resultados obtenidos por el método directo de Montecarlo y por el método del muestreo estratificado.

```{r tabla-de-resultados-est}
knitr::kable(
  data.frame(
    `Método` = c(
      "Directo",
      "Estratificado proporcional",
      "Estratificado óptimo"
    ),
    `Estimación` = c(
      estimacion_directo,
      estimacion_estratificado_proporcional,
      estimacion_estratificado_optimo
    ),
    Varianza = c(
      varianza_directo,
      varianza_estratificado_proporcional,
      varianza_estratificado_optimo
    ),
    Coste = c(
      coste_directo,
      coste_estratificado_proporcional,
      coste_estratificado_optimo
    ),
    Eficiencia = c(
      eficiencia_directo,
      eficiencia_estratificado_proporcional,
      eficiencia_estratificado_optimo
    )
  ),
  digits = 10
)
```

Para este problema, la eficiencia del método del muestreo estratificado es mucho menor que la del método directo, ya que la reducción de varianza conseguida es anulada completamente por el aumento del coste en tiempo. En este caso, el Método Estratificado proporcional es mejor, ya que reduce la varianza en un factor de 10 y además es el que menor eficiencia tiene.



# Ejercicio 3

Estimarr el valor de: $$I =
  \int_{0}^{1} 
  \dfrac{e^{- \frac{x^2}{2}}}{\sqrt{x(1-x)}}
  \,dx  \simeq 2,646630906854468
$$

## Definición de la función g

```{r}
pi<- pi
g<- function(x){
  ( exp(-(x^2)/(2)))/(sqrt(x*(1-x)))
}
```

## Generación de valores aleatorios

```{r}
n <- 1e6
valores_g <- replicate(n , {
  x <- genera_valor_aleatorio()
  g(x)
}
)
```

## Estimación de I

```{r}
estimacion <- mean(valores_g)
```

El valor estimado de $I$ es $`r estimacion`$. Este método siempre dará
una aproximación, mejor o peor pero se trata de una aproximación.

Puesto que esto es un resultado aleatorio, hay que calcular siempre un
intervalo de confianza que permita acotar la variabilidad de la
estimación.

## Intervalo de confianza

```{r}
probabilidad_cobertura <- 0.997
alfa <- 1 - probabilidad_cobertura
percentil<- qnorm(1 - alfa / 2)
error_estandar <- sqrt(var(valores_g)/n)
intervalo_confianza <- estimacion + c(-1,1) * error_estandar * percentil
```

Luego, un intervalo de confianza con probabilidad de cobertura 0.997 es
( $`r intervalo_confianza`$ )

# Ejercicio 4

Estimar el valor de: $$I =
  \int_{0}^{\infty} 
  \dfrac{e^{- \frac{x^2}{2}}}{\sqrt{x}}
  \,dx \simeq  2.1558005$$

Para poder estimar la integral a partir de la generación de valores
uniformes en $(0, 1)$ realizamos el siguiente cambio de variable: $$x = \dfrac{1}{1-U} - 1$$

Así, obtenemos la siguiente integral:

$$I =
  \int_{0}^{\infty} 
  \dfrac{e^{- \frac{x^2}{2}}}{\sqrt{x}}
  \,dx  = 
   \int_{0}^{1} 
  \dfrac{e^{- \frac{\big(\frac{1}{1-u}-1\big)^2}{2}}}{\sqrt{\frac{1}{1-u}-1}} \cdot \frac{1}{(1-u)^2}
  \, du = 
   \int_{0}^{1} 
  \dfrac{e^{- \frac{\big(\frac{1}{1-u}-1\big)^2}{2}}}{\sqrt{\frac{u}{1-u}}\cdot(1-u)^2}  
  \, du$$

## Definición de la función g

```{r}
g<- function(x){
  ( exp(-(((1/(1-x))-1)^2)/2))/(sqrt(x/(1-x))*(1-x)^2)
}
```

## Generación de valores aleatorios

```{r}
n <- 1e6
valores_g <- replicate(n , {
  x <- genera_valor_aleatorio()
  g(x)
}
)
```

## Estimación de I

```{r}
estimacion <- mean(valores_g)
```

El valor estimado de $I$ es $`r estimacion`$. Este método siempre dará
una aproximación, mejor o peor pero se trata de una aproximación.

Puesto que esto es un resultado aleatorio, hay que calcular siempre un
intervalo de confianza que permita acotar la variabilidad de la
estimación.

## Intervalo de confianza

```{r}
probabilidad_cobertura <- 0.997
alfa <- 1 - probabilidad_cobertura
percentil<- qnorm(1 - alfa / 2)
error_estandar <- sqrt(var(valores_g)/n)
intervalo_confianza <- estimacion + c(-1,1) * error_estandar * percentil
```

Luego, un intervalo de confianza con probabilidad de cobertura 0.997 es
( $`r intervalo_confianza`$ )

# Ejercicio 11

```{r configuracion, include=FALSE}
precision_resultados <- 5
```

Consideremos el grafo no dirigido. Se tiene que las longitudes
$X_i \sim U(0,a_i),$ con $a_1=1,a_2=2,a_3=3,a_4=1,a_5=2$, son
independientes. Si denotamos $X=(X_1,\dots , X_5)$, entonces el valor
$l=E[g(X)]$, con
\[g(X)= min (X_1 + X_4 , X_1+X_3 + X_5 , X_2 + X_3 + X_4 , X_2 + X_5 )\]

es el valor esperado de la longitud del camino más corto entre los nodos
A y D.

Se pide: *Estimar el valor de l*

```{r  echo=FALSE}
knitr::kable(
  data.frame(
    Tarea = paste(
      1:4,
      c(
        "A",
        "B",
        "C",
        "D"
      )
    ),
    Dependencias = c(
      "Ninguna",
      "Ninguna",
      "Ninguna",
      "Ninguna"
    )
  )
)
```

## Gráfo no dirigido

Podemos representar entonces el proyecto mediante el siguiente diagrama
en el que los nodos representan las tareas y los ejes las longitudes
entre ellas:

![](images/Captura%20de%20pantalla%202021-11-16%20a%20las%2014.14.55.png)



```{r PERT-diagrama, echo=FALSE, message=FALSE}
library(ggdag)
library(dagitty)
dag <- dagitty::dagitty('dag {
  A [latent, pos="0, 0"]
  B [latent, pos="1, 1"]
  C [latent, pos="1, -1"]
  D [latent, pos="2, 0"]
  A -> B
  B -> C
  A -> C
  B -> D
  C -> D
  A <- B
  B <- C
  A <- C
  B <- D
  C <- D
  }')

dag %>%
  tidy_dagitty() %>%
  ggdag() +
  labs(x = "", y = "") +
  scale_x_continuous(breaks = NULL, limits = c(-.25, 2.25)) +
  scale_y_continuous(breaks = NULL, limits = c(-1.25, 1.25))
```

-   A-B: X_1
-   A-C: X_2
-   B-C: X_3
-   B-D: X_4
-   C-D: X_5

Las longitudes medias son las siguientes:


```{r}
a<-c(1,2,3,1,2)
longitudes_medias<- a
```


## Objetivo

Consideramos que realmente se tienelas longitudes $X_i \sim U(0,a_i),$
con $a_1=1,a_2=2,a_3=3,a_4=1,a_5=2$, son independientes.

**Nuestro problema es, entonces, estimar** $l=E[g(X)]$ bajo esas
condiciones,con
$$ g(X)= min (X_1 + X_4 , X_1+X_3 + X_5 , X_2 + X_3 + X_4 , X_2 + X_5 )$$

Queremos estimar el valor de la longitud del camino mas corto entre los
nodos A y D

## Estimación con Método de montecarlo

### Generación

Para realizar esa estimación mediante el método de Montecarlo, definimos en primer lugar una función que genera la longitud aleatoria de cada uno de los caminos entre los nodos A y D y una función que calcula el valor de $g(x)$.

```{r generacion-1}
genera_vector_aleatorio <- function(l) {
  x_1 <- runif(1,min=0,max=l[1])
  x_2 <- runif(1,min=0,max=l[2])
  x_3 <- runif(1,min=0,max=l[3])
  x_4 <- runif(1,min=0,max=l[4])
  x_5 <- runif(1,min=0,max=l[5])
  return (c(x_1,x_2,x_3,x_4,x_5))
  }
  
g<- function(x) { 
  x_1<- x[1]
  x_2<- x[2]
  x_3<- x[3]
  x_4<- x[4]
  x_5<- x[5]
  min(x_1+x_4,x_1+x_3+x_5,x_2+x_3+x_4,x_2+x_5)
}
```

### Replicación

A continuación, replicamos una cantidad de veces parametrizada por la
variable `n` el proceso de generar duraciones aleatorias de las tareas,
según la distribución exponencial que corresponda, y calcular la
duración total del proyecto.

```{r replicacion-1}
n <- 1e5

valores_g <- replicate(n, {
  x<-genera_vector_aleatorio(longitudes_medias)
  g(x)
})
```

### Estimación

Finalmente, calculamos una estimación y un intervalo de confianza para
la longitud mínima esperada entre los nodos A y D.

```{r estimacion-1}
estimacion_longitud <- mean(valores_g) # estimación

# IC
probabilidad_cobertura <- 0.997
alfa <- 1 - probabilidad_cobertura
percentil <- qnorm(1 - alfa / 2)
error_estandar <- sqrt(var(longitudes_medias) / n)
intervalo_confianza <-
  estimacion_longitud + c(-1, 1) * error_estandar * percentil
```

### Solución

Hemos obtenido entonces una estimación de
$`r round(estimacion_longitud , digits = precision_resultados)`$
la longitud media del camino más corto entre los nodos A y D, siendo
$(`r round(intervalo_confianza, digits = precision_resultados)`)$ un
intervalo de confianza con probabilidad de cobertura
$`r probabilidad_cobertura`$.
